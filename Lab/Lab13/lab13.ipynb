{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerating Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fusing Convolution/Linear & BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\miniconda3\\envs\\spiking\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 2.5193, -0.2550, -0.3699],\n",
       "         [-0.0582,  0.7176, -0.4852],\n",
       "         [ 0.6087, -0.5791,  1.7046],\n",
       "         [ 1.4373,  0.2971,  0.6564]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.5657,  0.4022,  1.5948, -1.5223], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.8613,  1.7043,  1.3866, -0.1287], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0949,  0.0348,  0.4146, -1.8652], requires_grad=True),\n",
       " tensor([-0.9676,  0.1816,  1.1661, -0.9641]),\n",
       " tensor([4.9728, 0.8033, 2.8209, 2.1920]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(3, 4)\n",
    "bn1d = nn.BatchNorm1d(4)\n",
    "nn.init.normal_(linear.weight)\n",
    "nn.init.normal_(linear.bias)\n",
    "nn.init.normal_(bn1d.weight)\n",
    "nn.init.normal_(bn1d.bias)\n",
    "dataset = torch.randn(200, 3)\n",
    "for i in range(10):\n",
    "    x = dataset[i*20:(i+1)*20]\n",
    "    bn1d(linear(x))\n",
    "\n",
    "linear.weight, linear.bias, bn1d.weight, bn1d.bias, bn1d.running_mean, bn1d.running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1633,  3.8841, -2.0963, -1.6851],\n",
       "        [-0.8841,  2.8700, -1.7901, -1.6802]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.eval(),bn1d.eval()\n",
    "x = torch.randn(2, 3)\n",
    "bn1d(linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1633,  3.8841, -2.0963, -1.6851],\n",
       "        [-0.8841,  2.8700, -1.7901, -1.6802]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = (bn1d.weight*linear.weight.T/(torch.sqrt(bn1d.running_var+bn1d.eps))).T\n",
    "bias = bn1d.weight*(linear.bias-bn1d.running_mean)/(torch.sqrt(bn1d.running_var+bn1d.eps)) + bn1d.bias\n",
    "fused_linear = nn.Linear(3, 4)\n",
    "fused_linear.weight.data.copy_(weight),fused_linear.bias.data.copy_(bias)\n",
    "fused_linear.eval()\n",
    "fused_linear(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `torch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)`\n",
    "\n",
    "|input -other| $\\le$ atol + rtol $\\times$ |other|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(bn1d(linear(x)), fused_linear(x),atol=0,rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[[[-0.3755, -1.9969, -1.7692],\n",
       "           [-0.6517,  0.6055, -0.6112],\n",
       "           [ 1.2118, -2.6311, -0.8324]],\n",
       " \n",
       "          [[ 0.2619,  1.2034, -1.6152],\n",
       "           [-1.3033,  0.4315,  1.4261],\n",
       "           [ 0.6579, -1.7968, -1.5334]],\n",
       " \n",
       "          [[-1.7177,  0.9733,  0.3640],\n",
       "           [-0.1917, -1.1114,  0.4718],\n",
       "           [-0.7481, -1.1785, -0.3109]]],\n",
       " \n",
       " \n",
       "         [[[ 0.3502,  1.4771,  0.7519],\n",
       "           [ 0.7774, -0.7319, -1.2932],\n",
       "           [ 0.2290,  1.0232, -1.4021]],\n",
       " \n",
       "          [[-1.3428, -0.9688, -0.2844],\n",
       "           [ 0.1926, -0.4981,  0.0641],\n",
       "           [ 1.8073, -0.2913, -0.8414]],\n",
       " \n",
       "          [[ 0.1951, -0.2776, -0.6255],\n",
       "           [-0.2391,  1.3289, -0.5593],\n",
       "           [-1.7043, -1.6380, -1.8700]]],\n",
       " \n",
       " \n",
       "         [[[-1.4260,  1.7896, -0.5829],\n",
       "           [ 1.1172, -0.8984,  2.4433],\n",
       "           [ 0.7376,  0.9756, -0.4699]],\n",
       " \n",
       "          [[-0.4453, -0.9966, -0.4681],\n",
       "           [-1.6308, -0.7064,  0.1754],\n",
       "           [ 1.6604, -0.4524, -0.3656]],\n",
       " \n",
       "          [[ 1.5782, -0.5151,  1.4081],\n",
       "           [ 0.9988,  2.2140, -1.0888],\n",
       "           [ 0.0303, -1.2143, -0.0275]]],\n",
       " \n",
       " \n",
       "         [[[-0.0476, -0.0972, -0.5671],\n",
       "           [-0.3342,  2.2359, -0.3019],\n",
       "           [ 0.5040,  1.0464,  1.1501]],\n",
       " \n",
       "          [[-0.7535, -0.5710, -0.5639],\n",
       "           [ 0.1032, -1.0088, -0.7962],\n",
       "           [ 1.3035, -1.1425, -1.3378]],\n",
       " \n",
       "          [[-0.9856, -0.6419, -0.5323],\n",
       "           [-0.5176,  2.3117, -1.2901],\n",
       "           [ 0.5620, -0.7362,  1.4240]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.7548, -1.9660, -0.2960,  1.6803], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.3114, -1.7288, -0.6772, -1.2201], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 1.0351, -0.8380,  1.5426, -1.0766], requires_grad=True),\n",
       " tensor([-1.1938, -1.2784, -0.1219,  1.1513]),\n",
       " tensor([22.2461, 15.8506, 21.3042, 16.7903]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 4, 3, 1, 1)\n",
    "bn2d = nn.BatchNorm2d(4)\n",
    "nn.init.normal_(conv.weight)\n",
    "nn.init.normal_(conv.bias)\n",
    "nn.init.normal_(bn2d.weight)\n",
    "nn.init.normal_(bn2d.bias)\n",
    "dataset = torch.randn(200, 3, 10, 10)\n",
    "for i in range(10):\n",
    "    x = dataset[i*20:(i+1)*20]\n",
    "    bn2d(conv(x))\n",
    "\n",
    "conv.weight, conv.bias, bn2d.weight, bn2d.bias, bn2d.running_mean, bn2d.running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv_bn(conv_weight, conv_bias, bn_weight, bn_bias, bn_running_mean, bn_running_var, bn_eps):\n",
    "    shape = (bn_weight.shape[0], 1, 1, 1)\n",
    "    weight = (bn_weight.reshape(shape)*conv_weight /\n",
    "              (torch.sqrt(bn_running_var.reshape(shape)+bn_eps)))\n",
    "    bias = bn_weight*(conv_bias-bn_running_mean) / \\\n",
    "        (torch.sqrt(bn_running_var+bn_eps)) + bn_bias\n",
    "    return weight, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = conv.double()\n",
    "bn2d = bn2d.double()\n",
    "x = torch.randn(2, 3, 10, 10).double()\n",
    "fused_conv = nn.Conv2d(3, 4, 3, 1, 1).double()\n",
    "conv.eval(),bn2d.eval(),fused_conv.eval()\n",
    "weight, bias = fuse_conv_bn(conv.weight, conv.bias, bn2d.weight,\n",
    "                            bn2d.bias, bn2d.running_mean, bn2d.running_var, bn2d.eps)\n",
    "fused_conv.weight.data.copy_(weight), fused_conv.bias.data.copy_(bias)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(bn2d(conv(x)), fused_conv(x),atol=0,rtol=1e-12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `torch.nn.utils.fusion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['copy',\n",
       " 'fuse_conv_bn_eval',\n",
       " 'fuse_conv_bn_weights',\n",
       " 'fuse_linear_bn_eval',\n",
       " 'fuse_linear_bn_weights',\n",
       " 'torch']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils import fusion\n",
    "[k for k in dir(fusion) if not k.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1d = torch.randn(2,3)\n",
    "torch.allclose(bn1d(linear(x1d)), fusion.fuse_linear_bn_eval(linear,bn1d)(x1d),atol=0,rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2d = torch.randn(2,3,2,2).double()\n",
    "torch.allclose(bn2d(conv(x2d)), fusion.fuse_conv_bn_eval(conv,bn2d)(x2d),atol=0,rtol=1e-12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `torch.FX`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FX is a toolkit for developers to use to transform `nn.Module` instances. FX consists of three main components: a **symbolic tracer**, an **intermediate representation**, and **Python code generation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (conv1): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (nested): Sequential(\n",
       "    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (wrapped): WrappedBatchNorm(\n",
       "    (mod): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WrappedBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mod = nn.BatchNorm2d(1)\n",
    "    def forward(self, x):\n",
    "        return self.mod(x)\n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(1)\n",
    "        self.conv2 = nn.Conv2d(1, 1, 1)\n",
    "        self.nested = nn.Sequential(\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Conv2d(1, 1, 1),\n",
    "        )\n",
    "        self.wrapped = WrappedBatchNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.nested(x)\n",
    "        x = self.wrapped(x)\n",
    "        return x\n",
    "\n",
    "model = M()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [#users=1] = placeholder[target=x]\n",
      "    %conv1 : [#users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %bn1 : [#users=1] = call_module[target=bn1](args = (%conv1,), kwargs = {})\n",
      "    %conv2 : [#users=1] = call_module[target=conv2](args = (%bn1,), kwargs = {})\n",
      "    %nested_0 : [#users=1] = call_module[target=nested.0](args = (%conv2,), kwargs = {})\n",
      "    %nested_1 : [#users=1] = call_module[target=nested.1](args = (%nested_0,), kwargs = {})\n",
      "    %wrapped_mod : [#users=1] = call_module[target=wrapped.mod](args = (%nested_1,), kwargs = {})\n",
      "    return wrapped_mod\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    bn1 = self.bn1(conv1);  conv1 = None\n",
      "    conv2 = self.conv2(bn1);  bn1 = None\n",
      "    nested_0 = getattr(self.nested, \"0\")(conv2);  conv2 = None\n",
      "    nested_1 = getattr(self.nested, \"1\")(nested_0);  nested_0 = None\n",
      "    wrapped_mod = self.wrapped.mod(nested_1);  nested_1 = None\n",
      "    return wrapped_mod\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "traced_model = torch.fx.symbolic_trace(model)\n",
    "print(traced_model.graph)\n",
    "print(traced_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (conv1): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (nested): Module(\n",
       "    (1): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.fx.experimental.optimization\n",
    "fused_model = torch.fx.experimental.optimization.fuse(model)\n",
    "fused_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 1, 1)\n",
    "torch.allclose(fused_model(x), model(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `torch.JIT`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses dynamic computational graphs.\n",
    "- more flexible.\n",
    "- debug friendly.\n",
    "- generally slower.\n",
    "\n",
    "\n",
    "![pic](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracing & Scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features_lits, out_fearures) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.ModuleList()\n",
    "        for i in range(len(hidden_features_lits)):\n",
    "            if i == 0:\n",
    "                self.net.append(\n",
    "                    nn.Linear(in_features, hidden_features_lits[i]))\n",
    "            else:\n",
    "                self.net.append(\n",
    "                    nn.Linear(hidden_features_lits[i-1], hidden_features_lits[i]))\n",
    "\n",
    "        self.net.append(nn.Linear(hidden_features_lits[-1], out_fearures))\n",
    "        self.activation = 'relu'\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            f = F.relu\n",
    "        else:\n",
    "            f = F.tanh\n",
    "        for layer in self.net:\n",
    "            x = f(layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  net = self.net\n",
      "  _3 = getattr(net, \"3\")\n",
      "  net0 = self.net\n",
      "  _2 = getattr(net0, \"2\")\n",
      "  net1 = self.net\n",
      "  _1 = getattr(net1, \"1\")\n",
      "  net2 = self.net\n",
      "  _0 = getattr(net2, \"0\")\n",
      "  input = torch.relu((_0).forward(x, ))\n",
      "  input0 = torch.relu((_1).forward(input, ))\n",
      "  input1 = torch.relu((_2).forward(input0, ))\n",
      "  return torch.relu((_3).forward(input1, ))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLP(3, [4, 5, 6], 7)\n",
    "x = torch.arange(6).reshape(2, 3).float()\n",
    "traced_mlp_model = torch.jit.trace(mlp_model, x)\n",
    "print(traced_mlp_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionGate(nn.Module):\n",
    "    def forward(self,x):\n",
    "        if x.sum()>0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  return x\n",
      "\n",
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  return torch.neg(x)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssj\\AppData\\Local\\Temp\\ipykernel_26144\\672435724.py:3: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.sum()>0:\n"
     ]
    }
   ],
   "source": [
    "decision_gate = DecisionGate()\n",
    "x = torch.arange(6).reshape(2,3).float()\n",
    "traced_gate = torch.jit.trace(decision_gate, x)\n",
    "print(traced_gate.code)\n",
    "traced_gate_negative_x = torch.jit.trace(decision_gate, -x)\n",
    "print(traced_gate_negative_x.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[-0., -1., -2.],\n",
      "        [-3., -4., -5.]])\n"
     ]
    }
   ],
   "source": [
    "print(decision_gate(-x))\n",
    "print(traced_gate(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  if bool(torch.gt(torch.sum(x), 0)):\n",
      "    _0 = x\n",
      "  else:\n",
      "    _0 = torch.neg(x)\n",
      "  return _0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted_gate = torch.jit.script(decision_gate)\n",
    "print(scripted_gate.code)\n",
    "scripted_gate(x),scripted_gate(-x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing a `ScriptModule` will clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph. By default, forward will be preserved, as well as attributes & methods specified in preserved_attrs. Additionally, any attribute that is modified within a preserved method will be preserved.\n",
    "\n",
    "Freezing currently only accepts ScriptModules that are in eval mode.\n",
    "\n",
    "Freezing applies generic optimization that will speed up your model regardless of machine. To further optimize using server-specific settings, run `optimize_for_inference` after freezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(traced_mlp_model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_mlp_model.eval()\n",
    "freezed_mlp_model = torch.jit.freeze(traced_mlp_model)\n",
    "len(list(freezed_mlp_model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  input = torch.linear(x, CONSTANTS.c0, CONSTANTS.c1)\n",
      "  input0 = torch.relu(input)\n",
      "  input1 = torch.linear(input0, CONSTANTS.c2, CONSTANTS.c3)\n",
      "  input2 = torch.relu(input1)\n",
      "  input3 = torch.linear(input2, CONSTANTS.c4, CONSTANTS.c5)\n",
      "  input4 = torch.relu(input3)\n",
      "  input5 = torch.linear(input4, CONSTANTS.c6, CONSTANTS.c7)\n",
      "  return torch.relu(input5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(freezed_mlp_model.code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see the [Loading a PyTorch Model in C++](https://pytorch.org/tutorials/advanced/cpp_export.html) tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuse pointwise operations\n",
    "\n",
    "Pointwise operations (elementwise addition, multiplication, math functions - sin(), cos(), sigmoid() etc.) can be fused into a single kernel to amortize memory access time and kernel launch time.\n",
    "\n",
    "PyTorch JIT can fuse kernels automatically, although there could be additional fusion opportunities not yet implemented in the compiler, and not all device types are supported equally.\n",
    "\n",
    "Pointwise operations are memory-bound, for each operation PyTorch launches a separate kernel. Each kernel loads data from the memory, performs computation (this step is usually inexpensive) and stores results back into the memory.\n",
    "\n",
    "Fused operator launches only one kernel for multiple fused pointwise ops and loads/stores data only once to the memory. This makes JIT very useful for activation functions, optimizers, custom RNN cells etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GPU specific optimizations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable cuDNN auto-tuner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA cuDNN supports many algorithms to compute a **convolution**. Autotuner runs a short benchmark and selects the kernel with the best performance on a given hardware for a given input size.\n",
    "\n",
    "For convolutional networks (other types currently not supported), enable cuDNN autotuner before launching the training loop by setting:\n",
    "\n",
    "```python\n",
    "torch.backends.cudnn.benchmark = True\n",
    "```\n",
    "\n",
    "- the auto-tuner decisions may be non-deterministic; different algorithm may be selected for different runs.\n",
    "- in some rare cases, such as with highly variable input sizes, it’s better to run convolutional networks with autotuner disabled to avoid the overhead associated with algorithm selection for each input size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoid unnecessary CPU-GPU synchronization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoid unnecessary synchronizations, to let the CPU run ahead of the accelerator as much as possible to make sure that the accelerator work queue contains many operations.\n",
    "\n",
    "When possible, avoid operations which require synchronizations, for example:\n",
    "\n",
    "- `print(cuda_tensor)`\n",
    "- `cuda_tensor.item()`\n",
    "- memory copies: `tensor.cuda()`, `cuda_tensor.cpu()` and equivalent `tensor.to(device)` calls\n",
    "- `cuda_tensor.nonzero()`\n",
    "- python control flow which depends on results of operations performed on CUDA tensors e.g. `if (cuda_tensor != 0).all()`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tensors directly on the target device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling `torch.rand(size).cuda()` to generate a random tensor, produce the output directly on the target device: `torch.rand(size, device=torch.device('cuda'))`.\n",
    "\n",
    "This is applicable to all functions which create new tensors and accept device argument: `torch.rand()`, `torch.zeros()`, `torch.full()` and similar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use mixed precision and AMP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed precision leverages Tensor Cores and offers up to 3x overall speedup on Volta and newer GPU architectures. To use Tensor Cores AMP should be enabled and matrix/tensor dimensions should satisfy requirements for calling kernels that use Tensor Cores.\n",
    "\n",
    "To use Tensor Cores:\n",
    "- set sizes to multiples of 8 (to map onto dimensions of Tensor Cores)\n",
    "  - see [Deep Learning Performance Documentation](https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance) for more details and guidelines specific to layer type\n",
    "  - if layer size is derived from other parameters rather than fixed, it can still be explicitly padded e.g. vocabulary size in NLP models\n",
    "- enable AMP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Automatic Mixed Precision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.cuda.amp` provides convenience methods for mixed precision, where some operations use the `torch.float32`(`float`) datatype and other operations use `torch.float16` (`half`). Some ops, like linear layers and convolutions, are much faster in `float16` or `bfloat16`. Other ops, like reductions, often require the dynamic range of `float32`. Mixed precision tries to match each op to its appropriate datatype, which can reduce your network’s runtime and memory footprint.\n",
    "\n",
    "Ordinarily, “automatic mixed precision training” uses `torch.autocast` and `torch.cuda.amp.GradScaler` together.\n",
    "\n",
    "```python\n",
    "def make_model(in_size, out_size, num_layers):\n",
    "    layers = []\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(torch.nn.Linear(in_size, in_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Linear(in_size, out_size))\n",
    "    return torch.nn.Sequential(*tuple(layers))\n",
    "\n",
    "\n",
    "batch_size = 512 # Try, for example, 128, 256, 513.\n",
    "in_size = 4096\n",
    "out_size = 4096\n",
    "num_layers = 3\n",
    "num_batches = 50\n",
    "epochs = 3\n",
    "\n",
    "# Creates data in default precision.\n",
    "# The same data is used for both default and mixed precision trials below.\n",
    "# You don't need to manually change inputs' ``dtype`` when enabling mixed precision.\n",
    "data = [torch.randn(batch_size, in_size, device=\"cuda\") for _ in range(num_batches)]\n",
    "targets = [torch.randn(batch_size, out_size, device=\"cuda\") for _ in range(num_batches)]\n",
    "\n",
    "loss_fn = torch.nn.MSELoss().cuda()\n",
    "```\n",
    "\n",
    "##### Default Precision\n",
    "\n",
    "Without `torch.cuda.amp`, the following simple network executes all ops in default precision (`torch.float32`):\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        output = net(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "```\n",
    "\n",
    "##### Adding torch.autocast\n",
    "\n",
    "Instances of `torch.autocast` serve as context managers that allow regions of your script to run in mixed precision.\n",
    "\n",
    "```python\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    output = net(input)\n",
    "    # output is float16 because linear layers ``autocast`` to float16.\n",
    "    assert output.dtype is torch.float16\n",
    "\n",
    "    loss = loss_fn(output, target)\n",
    "    # loss is float32 because ``mse_loss`` layers ``autocast`` to float32.\n",
    "    assert loss.dtype is torch.float32\n",
    "\n",
    "# Exits ``autocast`` before backward().\n",
    "# Backward passes under ``autocast`` are not recommended.\n",
    "# Backward ops run in the same ``dtype`` ``autocast`` chose for corresponding forward ops.\n",
    "loss.backward()\n",
    "opt.step()\n",
    "opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding GradScaler\n",
    "\n",
    "Gradient scaling helps prevent gradients with small magnitudes from flushing to zero (“underflowing”) when training with mixed precision.\n",
    "\n",
    "torch.cuda.amp.GradScaler performs the steps of gradient scaling conveniently.\n",
    "\n",
    "```py\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    output = net(input)\n",
    "    loss = loss_fn(output, target)\n",
    "\n",
    "# Scales loss. Calls ``backward()`` on scaled loss to create scaled gradients.\n",
    "scaler.scale(loss).backward()\n",
    "\n",
    "# ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters.\n",
    "# If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called,\n",
    "# otherwise, optimizer.step() is skipped.\n",
    "scaler.step(opt)\n",
    "\n",
    "# Updates the scale for next iteration.\n",
    "scaler.update()\n",
    "\n",
    "opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspecting/modifying gradients (e.g., clipping)\n",
    "\n",
    "All gradients produced by `scaler.scale(loss).backward()` are scaled. If you wish to modify or inspect the parameters’ `.grad` attributes between `backward()` and `scaler.step(optimizer)`, you should unscale them first using `scaler.unscale_(optimizer)`.\n",
    "\n",
    "```py\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    output = net(input)\n",
    "    loss = loss_fn(output, target)\n",
    "scaler.scale(loss).backward()\n",
    "\n",
    "# Unscales the gradients of optimizer's assigned parameters in-place\n",
    "scaler.unscale_(opt)\n",
    "\n",
    "# Since the gradients of optimizer's assigned parameters are now unscaled, clips as usual.\n",
    "# You may use the same value for max_norm here as you would without gradient scaling.\n",
    "torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.1)\n",
    "\n",
    "scaler.step(opt)\n",
    "scaler.update()\n",
    "opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preallocate memory in case of variable input length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models for speech recognition or for NLP are often trained on input tensors with variable sequence length. Variable length can be problematic for PyTorch caching allocator and can lead to reduced performance or to unexpected out-of-memory errors. If a batch with a short sequence length is followed by an another batch with longer sequence length, then PyTorch is forced to release intermediate buffers from previous iteration and to re-allocate new buffers. This process is time consuming and causes fragmentation in the caching allocator which may result in out-of-memory errors.\n",
    "\n",
    "A typical solution is to implement preallocation. It consists of the following steps:\n",
    "\n",
    "1. generate a (usually random) batch of inputs with maximum sequence length (either corresponding to max length in the training dataset or to some predefined threshold)\n",
    "\n",
    "2. execute a forward and a backward pass with the generated batch, do not execute an optimizer or a learning rate scheduler, this step preallocates buffers of maximum size, which can be reused in subsequent training iterations\n",
    "\n",
    "3. zero out gradients\n",
    "\n",
    "4. proceed to regular training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization refers to techniques for doing both computations and memory accesses with lower precision data, usually int8 compared to floating point implementations. This enables performance gains in several important areas:\n",
    "\n",
    "- 4x reduction in model size;\n",
    "- 2-4x reduction in memory bandwidth;\n",
    "- 2-4x faster inference due to savings in memory bandwidth and faster compute with int8 arithmetic (the exact speed up varies depending on the hardware, the runtime, and the model).\n",
    "\n",
    "Fundamentally quantization means introducing approximations and the resulting networks have slightly less accuracy. These techniques attempt to minimize the gap between the full floating point accuracy and the quantized accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping function is what you might guess - a function that maps values from floating-point to integer space. A commonly used mapping function is a linear transformation given by $Q(r) = round(r/S+Z)$, where $r$ is the input and $S$, $Z$ are **quantization parameters**.\n",
    "\n",
    "To reconvert to floating point space, the inverse function is given by $\\tilde{r} = (Q(r) - Z)\\cdot S$.\n",
    "\n",
    "$\\tilde{r} \\ne r$, and their difference constitutes the quantization error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization Parameters\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping function is parameterized by the **scaling factor** $S$ and **zero-point** $Z$ .\n",
    "\n",
    "$S$ is simply the ratio of the input range to the output range $S = \\cfrac{\\beta - \\alpha}{\\beta_q - \\alpha_q}$.\n",
    "\n",
    "where [$\\alpha,\\beta$] is the clipping range of the input, i.e. the boundaries of permissible inputs. [$\\beta_q,\\alpha_q$] is the range in quantized output space that it is mapped to. For 8-bit quantization, the output range $\\beta_q - \\alpha_q \\le 2^8 -1$.\n",
    "\n",
    "$Z$ acts as a bias to ensure that a 0 in the input space maps perfectly to a 0 in the quantized space. $Z = -(\\cfrac{\\alpha}{S} - \\alpha_q)$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of choosing the input clipping range is known as calibration. The simplest technique (also the default in PyTorch) is to record the running mininmum and maximum values and assign them to $\\alpha$ and $\\beta$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three Types of Quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dynamic quantization (weights quantized with activations read/stored in floating point and quantized for compute)\n",
    "- static quantization (weights quantized, activations quantized, calibration required post training)\n",
    "- static quantization aware training (weights quantized, activations quantized, quantization numerics modeled during training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dynamic Quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "# original model\n",
    "# all tensors and computations are in floating point\n",
    "previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n",
    "                      /\n",
    "    linear_weight_fp32\n",
    "\n",
    "# dynamically quantized model\n",
    "# linear and LSTM weights are in int8\n",
    "previous_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32\n",
    "                      /\n",
    "    linear_weight_int8\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(lstm_for_demonstration(\n",
       "   (lstm): LSTM(8, 8)\n",
       " ),\n",
       " lstm_for_demonstration(\n",
       "   (lstm): DynamicQuantizedLSTM(8, 8)\n",
       " ))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.ao import quantization\n",
    "\n",
    "class lstm_for_demonstration(nn.Module):\n",
    "  def __init__(self,in_dim,out_dim,depth):\n",
    "     super(lstm_for_demonstration,self).__init__()\n",
    "     self.lstm = nn.LSTM(in_dim,out_dim,depth)\n",
    "\n",
    "  def forward(self,inputs,hidden):\n",
    "     out,hidden = self.lstm(inputs,hidden)\n",
    "     return out, hidden\n",
    "\n",
    "model_dimension=8\n",
    "sequence_length=20\n",
    "batch_size=1\n",
    "lstm_depth=1\n",
    "\n",
    "# random data for input\n",
    "inputs = torch.randn(sequence_length,batch_size,model_dimension)\n",
    "# hidden is actually is a tuple of the initial hidden state and the initial cell state\n",
    "hidden = (torch.randn(lstm_depth,batch_size,model_dimension), torch.randn(lstm_depth,batch_size,model_dimension))\n",
    "\n",
    "lstm_model = lstm_for_demonstration(model_dimension,model_dimension,lstm_depth)\n",
    "quantized_lstm = quantization.quantize_dynamic(\n",
    "    lstm_model, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "lstm_model,quantized_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (B): 3661\n",
      "Size (B): 2573\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (B):', os.path.getsize(\"temp.p\"))\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(lstm_model)\n",
    "print_size_of_model(quantized_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.79 ms ± 166 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lstm_model.forward(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952 µs ± 67.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit quantized_lstm.forward(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute value of output tensor values in the FP32 model is 0.17307 \n",
      "mean absolute value of output tensor values in the INT8 model is 0.17307\n",
      "mean absolute value of the difference between the output tensors is 0.00144 or 0.83 percent\n"
     ]
    }
   ],
   "source": [
    "# run the float model\n",
    "out1, hidden1 = lstm_model(inputs, hidden)\n",
    "mag1 = torch.mean(abs(out1)).item()\n",
    "print('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n",
    "\n",
    "# run the quantized model\n",
    "out2, hidden2 = quantized_lstm(inputs, hidden)\n",
    "mag2 = torch.mean(abs(out2)).item()\n",
    "print('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n",
    "\n",
    "# compare them\n",
    "mag3 = torch.mean(abs(out1-out2)).item()\n",
    "print('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-training Static Quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```py\n",
    "# original model\n",
    "# all tensors and computations are in floating point\n",
    "previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n",
    "                    /\n",
    "    linear_weight_fp32\n",
    "\n",
    "# statically quantized model\n",
    "# weights and activations are in int8\n",
    "previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n",
    "                      /\n",
    "    linear_weight_int8\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\miniconda3\\envs\\spiking\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define a floating point model where some layers could be statically quantized\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "\n",
    "# model must be set to eval mode for static quantization logic to work\n",
    "model_fp32.eval()\n",
    "\n",
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'x86' for server inference and 'qnnpack'\n",
    "# for mobile inference. Other quantization configurations such as selecting\n",
    "# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n",
    "# can be specified here.\n",
    "# Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n",
    "# for server inference.\n",
    "# model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\n",
    "model_fp32.qconfig = quantization.get_default_qconfig('x86')\n",
    "\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "# This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "model_fp32_fused = quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = quantization.prepare(model_fp32_fused)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "model_fp32_prepared(input_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_int8 = quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization Aware Training for Static Quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "# original model\n",
    "# all tensors and computations are in floating point\n",
    "previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n",
    "                      /\n",
    "    linear_weight_fp32\n",
    "\n",
    "# model with fake_quants for modeling quantization numerics during training\n",
    "previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32\n",
    "                           /\n",
    "   linear_weight_fp32 -- fq\n",
    "\n",
    "# quantized model\n",
    "# weights and activations are in int8\n",
    "previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n",
    "                     /\n",
    "   linear_weight_int8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\miniconda3\\envs\\spiking\\lib\\site-packages\\torch\\ao\\quantization\\utils.py:287: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define a floating point model where some layers could benefit from QAT\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.bn = torch.nn.BatchNorm2d(1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "\n",
    "# model must be set to eval for fusion to work\n",
    "model_fp32.eval()\n",
    "\n",
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'x86' for server inference and 'qnnpack'\n",
    "# for mobile inference. Other quantization configurations such as selecting\n",
    "# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n",
    "# can be specified here.\n",
    "# Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n",
    "# for server inference.\n",
    "# model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\n",
    "model_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
    "\n",
    "# fuse the activations to preceding layers, where applicable\n",
    "# this needs to be done manually depending on the model architecture\n",
    "model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32,\n",
    "    [['conv', 'bn', 'relu']])\n",
    "\n",
    "# Prepare the model for QAT. This inserts observers and fake_quants in\n",
    "# the model needs to be set to train for QAT logic to work\n",
    "# the model that will observe weight and activation tensors during calibration.\n",
    "model_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32_fused.train())\n",
    "\n",
    "def training_loop(model):\n",
    "    pass\n",
    "\n",
    "# run the training loop (not shown)\n",
    "training_loop(model_fp32_prepared)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, fuses modules where appropriate,\n",
    "# and replaces key operators with quantized implementations.\n",
    "model_fp32_prepared.eval()\n",
    "model_int8 = torch.ao.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Preparation for Eager Mode Static Quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to currently make some modifications to the model definition prior to Eager mode quantization. This is because currently quantization works on a module by module basis. Specifically, for all quantization techniques, the user needs to:\n",
    "\n",
    "1. Convert any operations that require output requantization (and thus have additional parameters) from functionals to module form (for example, using `torch.nn.ReLU` instead of `torch.nn.functional.relu`).\n",
    "\n",
    "2. Specify which parts of the model need to be quantized either by assigning `.qconfig` attributes on submodules or by specifying `qconfig_mapping`. For example, setting `model.conv1.qconfig = None` means that the `model.conv` layer will not be quantized, and setting `model.linear1.qconfig = custom_qconfig` means that the quantization settings for `model.linear1` will be using `custom_qconfig` instead of the global qconfig."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For static quantization techniques which quantize activations, the user needs to do the following in addition:\n",
    "\n",
    "1. Specify where activations are quantized and de-quantized. This is done using `QuantStub` and `DeQuantStub` modules.\n",
    "\n",
    "2. Use `FloatFunctional` to wrap tensor operations that require special handling for quantization into modules. Examples are operations like add and cat which require special handling to determine output quantization parameters.\n",
    "\n",
    "3. Fuse modules: combine operations/modules into a single module to obtain higher accuracy and performance. This is done using the `fuse_modules()` API, which takes in lists of modules to be fused. We currently support the following fusions: [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch-TensorRT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Torch-TensorRT](https://developer-blogs.nvidia.com/wp-content/uploads/2021/12/pytorch-torch-tensorrt.png)\n",
    "\n",
    "[Accelerating Inference Up to 6x Faster in PyTorch with Torch-TensorRT](https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference & Reading List\n",
    "- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "- [Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n",
    "- [Computational graphs in PyTorch and TensorFlow](https://towardsdatascience.com/computational-graphs-in-pytorch-and-tensorflow-c25cc40bdcd1)\n",
    "- [Introduction to Torchscript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)\n",
    "- [Loading a PyTorch Model in C++](https://pytorch.org/tutorials/advanced/cpp_export.html)\n",
    "- [TorchScript for Deployment](https://pytorch.org/tutorials/recipes/torchscript_inference.html)\n",
    "- [Deep Learning Performance Documentation](https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance)\n",
    "- [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)\n",
    "- [Quantization](https://pytorch.org/docs/stable/quantization.html)\n",
    "- [Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)\n",
    "- [Dynamic Quantization](https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html)\n",
    "- [Practical Quantization in PyTorch](https://pytorch.org/blog/quantization-in-practice/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
