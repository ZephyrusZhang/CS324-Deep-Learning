{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 05 Paradigm of deep learning code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly define\n",
    "learining_rate = 0.01\n",
    "epochs = 1000\n",
    "# Pass arguments via the command line\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "parser.add_argument('--epochs', type=int, default=1000)\n",
    "args, _ = parser.parse_known_args()\n",
    "learining_rate = args.learning_rate\n",
    "epochs = args.epochs\n",
    "# cmd/bash: python main.py --learning_rate 0.01 --epochs 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_dims, out_dims):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = nn.Parameter(torch.randn(in_dims, out_dims), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.randn(out_dims), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x @ self.w + self.b\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dims=2, hidden_dims=10, out_dims=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_dims = in_dims\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.out_dims = out_dims\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        # wrong example: self.list_net = [nn.Linear(2, 10), nn.Linear(10, 2)]\n",
    "        # right example: self.seq_net = nn.Sequential(nn.Linear(2, 10), nn.Linear(10, 2))\n",
    "        # or self.mlist_net = nn.ModuleList(nn.Linear(2, 10), nn.Linear(10, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the difference between `nn.Sequential` and `nn.ModuleList`:\n",
    "\n",
    "  - `Sequential` implements `forward` function, so it can be derectly called, e.g. `self.seq_net(x)`. `ModuleList` doest not implement `forward` function, and it's just a list for storing `Module`s.\n",
    "  - an OrderedDict of modules can be passed in `Sequential`.\n",
    "\n",
    "\n",
    "- the difference between `nn.ModuleList` and `list`.\n",
    "  \n",
    "  - modules in `nn.ModuleList` are properly registered and will be visible by all `Module` methods.\n",
    "  - modules in `list` are not registered, e.g., the parameters of modules in list cannot been visiable by `Optimizer`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Dataset and Dataloader\n",
    "\n",
    "\n",
    "A custom dataloader, take assignment 1 part 2/3 as example:\n",
    "\n",
    "We want to assess dataset via `for` loop, e.g. `for input, label in train_dloader`. How to implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.randn(100, 2)  # dataset with 100 samples and 2 features\n",
    "y = np.random.randint(0, 2, (100))  # labels\n",
    "\n",
    "\n",
    "# yield\n",
    "def data_generator(x, y, batch_size=32, shuffle=True):\n",
    "    n = x.shape[0]\n",
    "    if shuffle:\n",
    "        idx = np.random.permutation(n)\n",
    "    else:\n",
    "        idx = np.arange(n)\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch_idx = idx[i:i + batch_size]\n",
    "        yield x[batch_idx], y[batch_idx]\n",
    "\n",
    "\n",
    "# class \n",
    "class Dataloader(object):\n",
    "    def __init__(self, x, y, batch_size=32, shuffle=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n = x.shape[0]\n",
    "        self.idx = np.arange(self.n)\n",
    "        self.i = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idx)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i >= len(self):\n",
    "            raise StopIteration\n",
    "        batch_idx = self.idx[self.i * self.batch_size:(self.i + 1) * self.batch_size]\n",
    "        self.i += 1\n",
    "        return self.x[batch_idx], self.y[batch_idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and Dataloader in pytorch:\n",
    "\n",
    "We don't need implement `Dataloader`, we just need to give a custom `Dataset` implementation.\n",
    "\n",
    "For example, we have a dataset with the following folder structure:\n",
    "\n",
    "- train\n",
    "  - data \n",
    "    - sample 1\n",
    "    - sample 2\n",
    "    - ...\n",
    "  - label\n",
    "    - sample 1\n",
    "    - sample 2\n",
    "    - ...\n",
    "- val\n",
    "  - data \n",
    "    - sample 1\n",
    "    - sample 2\n",
    "    - ...\n",
    "  - label\n",
    "    - sample 1\n",
    "    - sample 2\n",
    "    - ...\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_path, transform=False):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform\n",
    "        self.image_paths = os.listdir(os.path.join(self.dataset_path, 'data'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.dataset_path, 'data', self.image_paths[idx])\n",
    "        # read image\n",
    "        image = ...\n",
    "        label_path = os.path.join(self.dataset_path, 'label', self.image_paths[idx])\n",
    "        # read label\n",
    "        label = ...\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is `transform` in `__init__` function? Format conversion and Dataset augmentation!\n",
    "\n",
    "For example, we are asked to do a classification for images. For different usages, `transform`s are usually diffrent.\n",
    "\n",
    "For training dataset, we expect that it contains dataset augmentation and necessary format conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "_normalize = transforms.Normalize(\n",
    "    (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(\n",
    "        32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomErasing(),\n",
    "    transforms.ToTensor(),\n",
    "    _normalize\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While for test/validation dataset, it just contains necessary format conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    _normalize\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is the fact of modifying the data of each channel/tensor so that the mean is zero and the standard deviation is one.(ref: [Why and How to normalize data â€“ Object detection on image in PyTorch Part 1](https://inside-machinelearning.com/en/why-and-how-to-normalize-data-object-detection-on-image-in-pytorch-part-1/#Normalizing_data))\n",
    "\n",
    "two main reasons\n",
    "\n",
    "- normalizing data includes them in the same range as our activation functions, usually between 0 and 1. This allows for less frequent non-zero gradients during training, and therefore the neurons in our network will learn faster.\n",
    "- by normalizing each channel so that they have the same distribution, we ensure that the channel information can be mixed and updated during the gradient descent (back propagation) using the same learning rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have implemented ours `Dataset`, so how to load this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = CustomDataset(dataset_path='train', transform=transform_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = CustomDataset(dataset_path='val', transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "for image, label in train_loader:\n",
    "    # do something\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are too many loss function for different tasks. But in fact, the loss function is just a `Module` that receives the output of the last layer and outputs a scalar.\n",
    "\n",
    "There we implement a MSELoss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.mean((y_pred - y_true) ** 2)\n",
    "        return loss\n",
    "\n",
    "\n",
    "criterion = MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch also provides many implemented loss, e.g., `L1Loss`, `MSELoss`, `CrossEntropyLoss`. You can refer to the official [documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Optimizer\n",
    "\n",
    "Pytorch implementes various optimization algorithms. Most commonly used methods are already supported. You can refer to [`torch.optim`](https://pytorch.org/docs/stable/optim.html).\n",
    "\n",
    "`Optimizer`s implemented by pytorch have two commonly used instance methods: `zero_grad` and `step`. `zero_grad` is used to clear gradient of all parameters it manages. And `step` updates parameters it manages according to its own rules. \n",
    "\n",
    "There we only demonstrate how to instance a `Optimizer` and what arguments it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "model = MLP()\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4, nesterov=False)\n",
    "\n",
    "# or \n",
    "optimizer = SGD([\n",
    "    {'params': model.fc1.parameters()},\n",
    "    {'params': model.fc2.parameters(), 'lr': 0.01}\n",
    "], lr=0.01, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `momentum` is consistent with the slides, the `weight_decay` is the coefficient of L2 regularization, and the `nesterov` is whether turn on nesterov momentum."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to adjust learning rate?\n",
    "\n",
    "`torch.optim.lr_scheduler` provides several methods to adjust the learning rate based on the number of epochs.\n",
    "\n",
    "There we show the `CosineAnnealingLR` and how do schedulers work with optimizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# optimizer: instanced before\n",
    "scheduler = CosineAnnealingLR(optimizer, eta_min=0, T_max=args.epochs)\n",
    "for epoch in range(epochs):\n",
    "    # train part\n",
    "    for input, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        ...\n",
    "        optimizer.step()\n",
    "\n",
    "    # test part\n",
    "    for input, target in test_loader:\n",
    "        ...\n",
    "    scheduler.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Train\n",
    "\n",
    "There we only show a template of training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # TODO calculate performance\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(val_loader, model, criterion):\n",
    "    model.eval()\n",
    "    for i, (images, target) in enumerate(val_loader):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # TODO calculate performance\n",
    "        performance = ...\n",
    "    return performance\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, model, criterion, optimizer)\n",
    "    performance = validate(test_loader, model, criterion)\n",
    "    # TODO compare the best performance with the current one, and save the current checkpoint\n",
    "    torch.save(model.state_dict(), 'save_path')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why `model.train()` and `model.eavl()`?\n",
    "\n",
    "Some `Module`s have diffrent behavior in training mode and test/evalate mode, e.g. `nn.Dropout`, `nn.BatchNorm2d`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n",
    "        super(BatchNorm2, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "            self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "        self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mean = x.mean((0, 2, 3))\n",
    "            var = x.var((0, 2, 3), unbiased=False)\n",
    "            n = x.numel() / x.size(1)\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var * n / (n - 1)\n",
    "                self.num_batches_tracked += 1\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        x = (x - mean[None, :, None, None]) / torch.sqrt(var[None, :, None, None] + self.eps)\n",
    "        if self.affine:\n",
    "            x = x * self.gamma[None, :, None, None] + self.beta[None, :, None, None]\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "class Sigmoid(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = 1 / (1 + torch.exp(-input))\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, = ctx.saved_tensors\n",
    "        grad_input = grad_output * output * (1 - output)\n",
    "        return grad_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
